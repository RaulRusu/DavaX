{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":107840,"databundleVersionId":13063981,"sourceType":"competition"},{"sourceId":12533634,"sourceType":"datasetVersion","datasetId":7912296},{"sourceId":481653,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":386072,"modelId":405231}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndataset_path = '/kaggle/input/identify-the-author/train/train.csv'\ndf = pd.read_csv(dataset_path)\nauthor_map = {\n    'EAP': 0,\n    'HPL': 1,\n    'MWS': 2\n}\n\ndf['author'] = df['author'].map(author_map)\ndf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:01:54.371843Z","iopub.execute_input":"2025-07-21T16:01:54.372156Z","iopub.status.idle":"2025-07-21T16:01:54.440524Z","shell.execute_reply.started":"2025-07-21T16:01:54.372132Z","shell.execute_reply":"2025-07-21T16:01:54.439811Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/identify-the-author/sample_submission/sample_submission.csv\n/kaggle/input/identify-the-author/test/test.csv\n/kaggle/input/identify-the-author/train/train.csv\n/kaggle/input/roberta-tf-1000/pytorch/default/1/best_model-r-1000-lower-reg.pt\n/kaggle/input/model1000/best_model.pt\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"            id                                               text  author\n0      id26305  This process, however, afforded me no means of...       0\n1      id17569  It never once occurred to me that the fumbling...       1\n2      id11008  In his left hand was a gold snuff box, from wh...       0\n3      id27763  How lovely is spring As we looked from Windsor...       2\n4      id12958  Finding nothing else, not even gold, the Super...       1\n...        ...                                                ...     ...\n19574  id17718  I could have fancied, while I looked at it, th...       0\n19575  id08973  The lids clenched themselves together as if in...       0\n19576  id05267  Mais il faut agir that is to say, a Frenchman ...       0\n19577  id17513  For an item of news like this, it strikes us i...       0\n19578  id00393  He laid a gnarled claw on my shoulder, and it ...       1\n\n[19579 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id26305</td>\n      <td>This process, however, afforded me no means of...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id17569</td>\n      <td>It never once occurred to me that the fumbling...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id11008</td>\n      <td>In his left hand was a gold snuff box, from wh...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id27763</td>\n      <td>How lovely is spring As we looked from Windsor...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id12958</td>\n      <td>Finding nothing else, not even gold, the Super...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19574</th>\n      <td>id17718</td>\n      <td>I could have fancied, while I looked at it, th...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19575</th>\n      <td>id08973</td>\n      <td>The lids clenched themselves together as if in...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19576</th>\n      <td>id05267</td>\n      <td>Mais il faut agir that is to say, a Frenchman ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19577</th>\n      <td>id17513</td>\n      <td>For an item of news like this, it strikes us i...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19578</th>\n      <td>id00393</td>\n      <td>He laid a gnarled claw on my shoulder, and it ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>19579 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom transformers import RobertaTokenizer, RobertaModel, RobertaConfig\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:01:54.442025Z","iopub.execute_input":"2025-07-21T16:01:54.442623Z","iopub.status.idle":"2025-07-21T16:02:02.577617Z","shell.execute_reply.started":"2025-07-21T16:01:54.442601Z","shell.execute_reply":"2025-07-21T16:02:02.577030Z"}},"outputs":[{"name":"stderr","text":"2025-07-21 16:01:59.814797: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753113719.838813     281 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753113719.845633     281 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"label_map = {author: i for i, author in enumerate(df['author'].unique())}\ndf['label'] = df['author'].map(label_map)\n\ntexts = df['text'].tolist()\nlabels = df['label'].tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:02:02.578380Z","iopub.execute_input":"2025-07-21T16:02:02.578833Z","iopub.status.idle":"2025-07-21T16:02:02.586578Z","shell.execute_reply.started":"2025-07-21T16:02:02.578814Z","shell.execute_reply":"2025-07-21T16:02:02.586059Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Split for training/testing\nX_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, stratify=labels, random_state=22)\n\n# Fit TF-IDF on train, transform both\nvectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,5), max_features=1000)\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# Reduce dimensionality\n# svd = TruncatedSVD(n_components=100, random_state=42)\n# X_train_svd = svd.fit_transform(X_train_tfidf)\n# X_test_svd = svd.transform(X_test_tfidf)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:02:02.587186Z","iopub.execute_input":"2025-07-21T16:02:02.587363Z","iopub.status.idle":"2025-07-21T16:02:05.759853Z","shell.execute_reply.started":"2025-07-21T16:02:02.587343Z","shell.execute_reply":"2025-07-21T16:02:05.759235Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\ndef tokenize(texts, tokenizer, max_length=256):\n    encodings = tokenizer(\n        texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt'\n    )\n    return encodings\n\ntrain_encodings = tokenize(X_train, tokenizer)\ntest_encodings = tokenize(X_test, tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:02:05.761307Z","iopub.execute_input":"2025-07-21T16:02:05.761571Z","iopub.status.idle":"2025-07-21T16:02:13.552934Z","shell.execute_reply.started":"2025-07-21T16:02:05.761547Z","shell.execute_reply":"2025-07-21T16:02:13.552146Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class HybridDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, tfidf_dense, labels, texts):\n        self.encodings = encodings\n        self.tfidf_dense = torch.tensor(tfidf_dense, dtype=torch.float32)\n        self.labels = torch.tensor(labels, dtype=torch.long)\n        self.texts = texts\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['tfidf_dense'] = self.tfidf_dense[idx]\n        item['labels'] = self.labels[idx]\n        item['texts'] = self.texts[idx]\n        return item\n        \nX_train_tfidf_dense = X_train_tfidf.toarray()\nX_test_tfidf_dense = X_test_tfidf.toarray()\n\ntrain_dataset = HybridDataset(train_encodings, X_train_tfidf_dense, y_train, X_train)\ntest_dataset = HybridDataset(test_encodings, X_test_tfidf_dense, y_test, X_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:02:13.554130Z","iopub.execute_input":"2025-07-21T16:02:13.554359Z","iopub.status.idle":"2025-07-21T16:02:13.822727Z","shell.execute_reply.started":"2025-07-21T16:02:13.554340Z","shell.execute_reply":"2025-07-21T16:02:13.822028Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class HybridClassifier(nn.Module):\n    def __init__(self, tfidf_dim, num_labels):\n        super().__init__()\n        self.config = RobertaConfig.from_pretrained('roberta-base', \n                                                    hidden_dropout_prob=0.15, \n                                                    attention_probs_dropout_prob=0.05)\n        self.transformer = RobertaModel.from_pretrained('roberta-base', config = self.config)\n        self.hidden_size = self.transformer.config.hidden_size\n        self.classifier = nn.Sequential(\n            nn.Linear(self.hidden_size + tfidf_dim, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 64),  # buffer layer\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, 3) \n        )\n    def forward(self, input_ids, attention_mask, tfidf_dense):\n        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n        cls_emb = outputs.last_hidden_state[:, 0, :]\n        combined = torch.cat((cls_emb, tfidf_dense), dim=1)\n        logits = self.classifier(combined)\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:02:13.823352Z","iopub.execute_input":"2025-07-21T16:02:13.823959Z","iopub.status.idle":"2025-07-21T16:02:13.830384Z","shell.execute_reply.started":"2025-07-21T16:02:13.823940Z","shell.execute_reply":"2025-07-21T16:02:13.829464Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HybridClassifier(tfidf_dim=1000, num_labels=3).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.02)\nloss_fn = nn.CrossEntropyLoss(label_smoothing=0.05)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n\ndef freeze_roberta(model):\n    for param in model.transformer.parameters():\n        param.requires_grad = False\n\ndef unfreeze_roberta(model):\n    for param in model.transformer.parameters():\n        param.requires_grad = True\n        \npatience = 2  # Number of epochs with no improvement before stopping\nbest_eval_loss = float('inf')\nepochs_no_improve = 0\n\nnum_frozen_epochs = 0\n\nfor epoch in range(20):  # You can use a large number here\n    # ---- Training ----\n    if epoch == 0:\n        freeze_roberta(model)\n    if epoch == num_frozen_epochs:\n        unfreeze_roberta(model)\n        \n    model.train()\n    total_loss = 0\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        tfidf_dense = batch['tfidf_dense'].to(device)\n        labels = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n        logits = model(input_ids, attention_mask, tfidf_dense)\n        loss = loss_fn(logits, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    avg_train_loss = total_loss / len(train_loader)\n\n    # ---- Evaluation ----\n    model.eval()\n    eval_loss = 0\n    all_preds = []\n    all_labels = []\n    all_probs = []\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=f\"Epoch {epoch+1} - Eval\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            tfidf_dense = batch['tfidf_dense'].to(device)\n            labels = batch['labels'].to(device)\n    \n            logits = model(input_ids, attention_mask, tfidf_dense)\n            loss = loss_fn(logits, labels)\n            eval_loss += loss.item()\n            \n            # For metrics:\n            probs = torch.softmax(logits, dim=-1).cpu().numpy()   # shape [batch, 3]\n            preds = np.argmax(probs, axis=1)                      # shape [batch]\n            all_probs.append(probs)\n            all_preds.append(preds)\n            all_labels.append(labels.cpu().numpy())\n    \n    avg_eval_loss = eval_loss / len(test_loader)\n    \n    # Concatenate all results\n    all_preds = np.concatenate(all_preds)\n    all_labels = np.concatenate(all_labels)\n    all_probs = np.concatenate(all_probs)\n    \n    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n    logloss = log_loss(all_labels, all_probs)\n    \n    print(f\"Epoch {epoch+1} - Training loss: {avg_train_loss:.4f} | Eval loss: {avg_eval_loss:.4f} | Macro F1: {macro_f1:.4f} | Logloss: {logloss:.4f}\")\n\n    print(f\"Epoch {epoch+1} - Training loss: {avg_train_loss:.4f} | Eval loss: {avg_eval_loss:.4f}\")\n\n    # ---- Early Stopping ----\n    if avg_eval_loss < best_eval_loss:\n        best_eval_loss = avg_eval_loss\n        epochs_no_improve = 0\n        # Optionally save the model's state_dict here if desired:\n        torch.save(model.state_dict(), \"best_model-r-1000-lower-reg.pt\")\n    else:\n        epochs_no_improve += 1\n        print(f\"No improvement for {epochs_no_improve} epochs.\")\n        if epochs_no_improve >= patience:\n            print(\"Early stopping!\")\n            break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"/kaggle/working/best_model-r-1000-lower-reg.pt\"))\n\nfreeze_roberta(model)\noptimizer = torch.optim.AdamW(model.classifier.parameters(), lr=2e-6)  # You can try a higher LR here\nepochs_no_improve = 0\nfor epoch in range(20):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        tfidf_dense = batch['tfidf_dense'].to(device)\n        labels = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n        logits = model(input_ids, attention_mask, tfidf_dense)\n        loss = loss_fn(logits, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    avg_train_loss = total_loss / len(train_loader)\n\n    # ---- Evaluation ----\n    model.eval()\n    eval_loss = 0\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=f\"Epoch {epoch+1} - Eval\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            tfidf_dense = batch['tfidf_dense'].to(device)\n            labels = batch['labels'].to(device)\n\n            logits = model(input_ids, attention_mask, tfidf_dense)\n            loss = loss_fn(logits, labels)\n            eval_loss += loss.item()\n    avg_eval_loss = eval_loss / len(test_loader)\n\n    print(f\"Epoch {epoch+1} - Training loss: {avg_train_loss:.4f} | Eval loss: {avg_eval_loss:.4f}\")\n\n    # ---- Early Stopping ----\n    if avg_eval_loss < best_eval_loss:\n        best_eval_loss = avg_eval_loss\n        epochs_no_improve = 0\n        # Optionally save the model's state_dict here if desired:\n        torch.save(model.state_dict(), \"best_model-header-o.pt\")\n    else:\n        epochs_no_improve += 1\n        print(f\"No improvement for {epochs_no_improve} epochs.\")\n        if epochs_no_improve >= patience:\n            print(\"Early stopping!\")\n            break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\nmodel.load_state_dict(torch.load(\"/kaggle/working/best_model-r-1000-lower-reg.pt\"))\nmodel.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        tfidf_dense = batch['tfidf_dense'].to(device)\n        labels = batch['labels'].to(device)\n        logits = model(input_ids, attention_mask, tfidf_dense)\n        preds = torch.argmax(logits, dim=-1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\nacc = accuracy_score(all_labels, all_preds)\nf1 = f1_score(all_labels, all_preds, average='macro')\nprint(f\"Test Accuracy: {acc:.4f}\")\nprint(f\"Test Macro F1: {f1:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\nmodel.load_state_dict(torch.load(\"/kaggle/working/best_model-r-1000-lower-reg.pt\"))\nmodel.eval()\n\nfrom torch.utils.data import DataLoader\n\ndef custom_collate(batch):\n    # All keys in item dict\n    batch_keys = batch[0].keys()\n    collated = {}\n    for key in batch_keys:\n        if key == 'texts':\n            # List of strings, just collect as list\n            collated[key] = [item[key] for item in batch]\n        else:\n            # Stack tensors (input_ids, tfidf_dense, labels, etc.)\n            collated[key] = torch.stack([item[key] for item in batch])\n    return collated\n\n# Use this in your DataLoader\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=32,\n    shuffle=False,\n    collate_fn=custom_collate  # <-- THIS LINE!\n)\n\nall_preds, all_labels, all_texts, all_lengths = [], [], [], []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        tfidf_dense = batch['tfidf_dense'].to(device)\n        labels = batch['labels'].to(device)\n        texts = batch['texts']  # Add this to your Dataset __getitem__ if not present\n\n        logits = model(input_ids, attention_mask, tfidf_dense)\n        preds = torch.argmax(logits, dim=-1)\n\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n        all_texts.extend(texts)\n        all_lengths.extend([len(t) for t in texts])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\n# Evaluate model and collect predictions, labels, texts, lengths as before...\n\n# Build the DataFrame for analysis\nresults_df = pd.DataFrame({\n    \"true_label\": all_labels,\n    \"pred_label\": all_preds,\n    \"text\": all_texts,\n    \"text_length\": all_lengths\n})\n\n# Add the length_bucket column BEFORE filtering\nresults_df['length_bucket'] = pd.cut(\n    results_df['text_length'],\n    bins=[0, 50, 100, 256, 512, 4096]\n)\n\n# Misclassifications\nmissed = results_df[results_df['true_label'] != results_df['pred_label']]\nprint(f\"\\nNumber of misclassified samples: {len(missed)}/{len(results_df)} ({len(missed)/len(results_df)*100:.2f}%)\")\nprint(\"True labels of misclassified samples:\\n\", missed['true_label'].value_counts())\nprint(\"Predicted labels of misclassified samples:\\n\", missed['pred_label'].value_counts())\n\n# Length analysis\nprint(\"\\nMisclassification rate by length bucket:\")\nerror_rate_by_bucket = missed['length_bucket'].value_counts(normalize=True)\nprint(error_rate_by_bucket)\n\n# Confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot()\nplt.show()\n\n# Show random misclassified examples\nprint(\"\\nSample misclassified examples:\")\nsampled = missed.sample(5, random_state=42)\nfor idx, row in sampled.iterrows():\n    print(f\"True: {row['true_label']}, Pred: {row['pred_label']}, Length: {row['text_length']}\")\n    print(row['text'][:250])  # Print first 250 chars\n    print(\"-\" * 50)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ntest_df = pd.read_csv(\"/kaggle/input/identify-the-author/test/test.csv\")\nids = test_df[\"id\"].tolist()\ntexts = test_df[\"text\"].tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example: TF-IDF + SVD for test\nX_test_tfidf = vectorizer.transform(texts)\nX_test_tfidf_dense = X_test_tfidf.toarray()\n# Example: Tokenize for RoBERTa/DistilRoBERTa\ntest_encodings = tokenizer(\n    texts, truncation=True, padding=True, max_length=256, return_tensors='pt'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nclass HybridTestDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, tfidf_dense):\n        self.encodings = encodings\n        self.tfidf_dense = torch.tensor(tfidf_dense, dtype=torch.float32)\n    def __len__(self):\n        return self.tfidf_dense.shape[0]\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['tfidf_dense'] = self.tfidf_dense[idx]\n        return item\n\ntest_dataset = HybridTestDataset(test_encodings, X_test_tfidf_dense)  # or X_test_tfidf if no SVD\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\n\nmodel.eval()\nall_probs = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        tfidf_dense = batch['tfidf_dense'].to(device)\n\n        logits = model(input_ids, attention_mask, tfidf_dense)\n        probs = torch.softmax(logits, dim=-1)\n        all_probs.append(probs.cpu().numpy())\n\nall_probs = np.concatenate(all_probs, axis=0)  # shape: (num_samples, 3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Map column indices to author labels\n# Or, for correct ordering, just do:\nauthor_labels = ['EAP', 'HPL', 'MWS']\n\nsubmission_df = pd.DataFrame(all_probs, columns=author_labels)\nsubmission_df.insert(0, 'id', ids)\n\neps = 1e-15\nsubmission_df[author_labels] = submission_df[author_labels].clip(eps, 1 - eps)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df.to_csv(\"submission-best_model-r-1000-lower-reg.csv\", index=False, float_format='%.15f')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}